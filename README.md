# Remi: Ingredient Classification and Recipe Recommendation System

## ğŸ“˜ Abstract
**Remi** is an AI-powered system that classifies ingredients in an image and recommends recipes you can make with them. We leverage a combination of modern computer vision techniques like **YOLOv11** for ingredient detection and **CLIP-based classification** for more specific ingredient identification. The detected ingredients are matched against a **recipe database** to suggest dishes that can be prepared with the available ingredients or alternatively fed into the **ChatGPT API**.

The system is designed to provide fast, accurate, and practical suggestions for users looking for inspiration in the kitchen.

---

## ğŸš€ Features
- **Ingredient Detection**: Uses **YOLOv11m** to detect general ingredient objects from images.  
- **Ingredient Classification**: Crops detected objects and classifies them using **CLIP** embeddings and a custom label list.  
- **Recipe Suggestion**: Matches the identified ingredients against a recipe database to suggest dishes that require the available ingredients.  
- **ChatGPT Integration**: Optionally use the **ChatGPT API** to generate creative recipe suggestions.  
- **Scalability**: Supports adding more datasets, labels, and custom recipes. Also is can be separated in to two parts: the detector and the clasifier and can be tuned independantly on each other.   

---

## ğŸ“· System Architecture / Workflow
```
             User Input (Image) 
                    â†“
          YOLOv11 Detection (Objects)
                    â†“
        Cropping & CLIP Ingredient Classification
                    â†“
          Recipe Matching (via database or ChatGPT)
                    â†“
      Recipe Suggestions 
```

---

## ğŸ“š Methodology

### ğŸ§‘â€ğŸ”¬ **1. Data Collection**
We sourced datasets for ingredients from various online sources, specifically from **Roboflow**, and combined them into a **unified COCO-like format**. This allowed us to train YOLOv11 on a larger and more diverse set of ingredient images. Additionally, since public datasets for packaged ingredients (like milk bottles, spice containers) are limited, we augmented the dataset with images containing objects like toothpaste and packaged products. 

This step was crucial since it allowed us to avoid the costly process of creating a custom-labeled dataset while ensuring the system could recognize and distinguish ingredient packaging from raw ingredients.

### âš™ï¸ **2. Object Detection with YOLOv11m**
We trained a **YOLOv11m** model to detect general ingredient objects in images. The model detects bounding boxes for objects like "onion", "carrot", or "tomato".

**Training details**:
- **Datasets**: Combined datasets from multiple online sources and Roboflow.  
- **Epochs**: **200** epochs with **16 batch size**.  
- **Augmentations**: Flips, color shifts, random noise, and other augmentations included in the datasets.  

### ğŸ–¼ï¸ **3. Ingredient Classification using CLIP**
After YOLOv11 detects objects, each detected object is **cropped** and passed into a **CLIP-based classification system**.

**Why CLIP?** CLIP is a multimodal model that can classify images based on text embeddings. We created a list of custom **ingredient labels** like "tomato", "sweet potato", "beer", and so on. Each image crop is classified into one of these categories using CLIP's image-to-text similarity scores.

### ğŸ² **4. Recipe Matching**
Once the ingredients are identified, they are matched against a **recipe database (CSV file)**. We filter recipes by checking if the detected ingredients are sufficient to create the recipe. If some ingredients are missing, the system informs the user which ingredients are required. 

Optionally, we can send the list of detected ingredients to **ChatGPT** to generate creative and unique recipe suggestions.

---

## ğŸ“Š Evaluation

### ğŸ” **1. Metrics**
We evaluated the ingredient detection system using the following metrics:
- **Precision**: Measures how accurate our predictions are (i.e., how many of the detected ingredients are correct).  
- **Recall**: Measures how many of the ground-truth ingredients were detected.  
- **mAP (Mean Average Precision)**: This metric gives a comprehensive view of precision and recall across different IoU thresholds.  

---

### ğŸ“Š **Model Evaluation Results**

| **IoU Threshold** | **Precision** | **Recall** |
|-------------------|---------------|------------|
| **0.1**           | 0.7034        | 0.8465     |
| **0.5**           | 0.5937        | 0.7951     |
| **0.75**          | 0.4884        | 0.7091     |
| **0.9**           | 0.3992        | 0.5785     |

> **What does this mean?** At **IoU=0.1**, the system identifies 84.65% of the ingredients in the image, and 70.34% of them are correctly classified with an overlap of at least 10%. As the IoU threshold increases, precision decreases, but this is expected since stricter thresholds require more precise localization.

---

## ğŸ½ï¸ **Recipe Recommendation System**

### **How it works:**
1ï¸âƒ£ **Input**: User uploads an image of ingredients.  
2ï¸âƒ£ **Detection**: YOLOv11m identifies the ingredients.  
3ï¸âƒ£ **Classification**: Each detected ingredient is classified (e.g., "onion", "garlic").  
4ï¸âƒ£ **Recipe Matching**: Ingredients are compared to recipes in the CSV or sent to **ChatGPT** for creative suggestions.  

---

## âš™ï¸ **How to Run It**

1ï¸âƒ£ **Install dependencies**:  
```bash
conda env create -f environment.yaml --name new_env_name
```

2ï¸âƒ£ **Run the pipeline**:  
Place your images in ğŸ“‚: `project/classification_pipeline/images`

Run the command:  
```bash
python main.py
```

3ï¸âƒ£ **View the classified images**:  
Classified images will be saved in ğŸ“‚: `project/classification_pipeline/results/annotated_images`

---

## ğŸ“‚ **File Structure**

```
project_root/
â”œâ”€â”€ chatGPT_API/               # GPT API, requires .env file with API key
â”œâ”€â”€ classification_pipeline/   
â”‚   â”œâ”€â”€ clip_model_pipeline/   
â”‚   â”œâ”€â”€ yolo_model_pipeline/  
â”œâ”€â”€ ingredients_config/        
â”œâ”€â”€ recipe_dataset/            # CSV dataset with recipes
â”œâ”€â”€ utils/                     # Helper scripts
â”œâ”€â”€ app.py                     # App for uploading images and classifying them
â”œâ”€â”€ README.md                  # This README file
â”œâ”€â”€ main.py                    # Main script to run the project
â””â”€â”€ paths_config.yaml          # Configurable paths for images, models, and outputs
```

This structure helps users and developers understand where to find key components of the system.

---

## ğŸ§ª **Future Work**
- **Ingredient Quantity**: Measure how much of each ingredient is present (not just "onion" but **"50g of onion"**).  
- **More Recipes**: Use an API like **Spoonacular** to get more recipe suggestions.  
- **Better Train Dataset**: Improve dataset quality, especially for packaged ingredient images.  

---

## ğŸ“˜ **Final Thoughts**
This README is designed to help users, instructors, and collaborators understand how **Remi** works. It explains the purpose, methodology, and usage clearly, while also offering insights into how to run and evaluate the system. Let us know if you'd like any part refined or expanded.

References

YOLOv11: An Overview of the Key Architectural Enhancements. Retrieved from [https://arxiv.org/abs/2410.17725]

Radford, A., et al. (2021). Learning Transferable Visual Models From Natural Language Supervision. Retrieved from [https://arxiv.org/abs/2103.00020]


